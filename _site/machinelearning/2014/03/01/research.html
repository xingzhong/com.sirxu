<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Recursive Bayesian estimation based on stochastic context-free grammar</title>
        <meta name="viewport" content="width=device-width">
        <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,600' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" href="/css/pure-min.css">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">
        
    </head>
    <body>
    	<div class="pure-g-r page">
    		<div class="pure-u-1-6">
    			<div class="sidebar" id="menu">
    				<div class="pure-menu pure-menu-open">
    					<a class="pure-menu-heading" href="/index.html"> Xingzhong </a>
    					<img class="avatar" alt="Xingzhong&#x27;s avatar" height="105" width="105" src="/assets/profile.jpg">
	    				<ul>
					        <li id="resume"><a href="/resume.html">Resume</a></li>
					        <li id="blog"><a href="/blog.html">Blog</a></li>
                  <li id="port"><a href="/portfolio.html">Portfolio</a></li>
					    </ul>
    				</div>
    			</div>
    		</div>
    		<div class="pure-u-5-6">
    			<div class="content">
    				<h2>Recursive Bayesian estimation based on stochastic context-free grammar</h2>
<p class="meta">01 Mar 2014</p>

<div class="post">
<h2>Time series analysis</h2>

<p>Many machine learning and statistic problem can be generalized as an approach to estimate one or series unknown probability distributions based on some known facts.
In a typical time series scenario, the observed data have natural temporal order which the data will become available one by one through time.
In order to take advantage of such ordering, the ideal estimation should performed recursively following the time.
The model as well as the embedded parameters will be updated incrementally through time.
And most important applications include filtering, predication, and smoothing can be derived under this model.</p>

<p>Mathematically, unless the underlying time series is constant, people used to assume it as a <strong>stochastic process</strong>. Many statistic models are proposed to try better characterize the process.
The most basic model only consider the level values. Typically such model assume the current sample have linear dependence with previous sample plus some white noise.
Three broad classes of practical importance are the <strong>autoregressive</strong> (AR) models, the <strong>integrated</strong> (I) models, and the <strong>moving average</strong> (MA) models.
Based on those models, there also have some combination models commonly used in practical applications include ARMA, ARIMA etc.</p>

<p>Beyond the linear relationship, there are models to represent the change of variance instead of values over time.
For example, the volatility clustering in financial time series tend to have heavy dependency among volatility <em>"large changes tend to be followed by large changes, of either sign, and small changes tend to be followed by small changes"</em>.
In order to capture and quantify the additional variances correlation over time (heteroskedasticity), researchers propose more advanced <strong>autoregressive conditional heteroskedasticity</strong> (ARCH) model.</p>

<p>Inspired from heteroskedasticity model, instead of modeling the observations directly, the more advanced model starts to capture the indirect statistics.
Such changing variance, though hidden from our observations, could also be modeled by some stochastic state process.
By allowing more hidden control variables as the system state, state space model can be used to generalized such assumptions.
The most common state space model on practical usage is <strong>hidden Markov model</strong> which assume the unobserved (hidden) states is a Markov process.</p>

<h2>Recursive Bayesian estimation</h2>

<p>In most statistical models, the parameterized states to be modeled are  <strong>posterior probability density functions</strong> conditioned on observations.
Specifically in time series, our mission is seeking a solution to estimate such pdf in recursive manner.
This optimal solution is typically intractable unless in linear assumption.
However, in most cases, it have optimal substructure over the time index. Therefore, it is possible to construct an efficient numerical solution through dynamical programming.</p>

<p>Once we go towards to state space model, we already assume their are two layers in the system. One is the observations <code>\( \mathbf{O} \)</code> which are input variables that we can directly observed from outside. And the other one is the hidden states <code>\( \mathbf{X} \)</code> which are assumed generate <code>\( \mathbf{O} \)</code> in the backyard.
Therefore, once we have <code>\( \mathbf{O} \)</code> in hand, we could based on that to infer who is the hidden state <code>\( \mathbf{X} \)</code> generate such <code>\( \mathbf{O} \)</code> probabilistically.</p>

<p>Notice that both <code>\( \mathbf{O} \)</code> and <code>\( \mathbf{X} \)</code> are time indexed process with sort of dependency on series.
So the inference of <code>\( X_i \)</code> is not only depend on <code>\( O_i \)</code>, but also depend on the past <code>\( X_{1:i-1} \)</code> and <code>\( O_{1:i-1} \)</code>.
Typically we assume the dependency between <code>\( X_i \)</code> and <code>\( O_{1:i-1} \)</code> are go through <code>\( X_{1:i-1} \)</code>.
Hence <code>\( \mathbf{X} \)</code> are conditional independent between each other.
Another assumption is <code>\( \mathbf{X} \)</code> have Markov property, which leads to
<code>\[
\mathbf{P}(X_k | X_{1:k-1}) = \mathbf{P}(X_k | X_{k-1})
\]</code></p>

<p>In above model, our goal is to recursively estimate <code>\( \mathbf{X} \)</code> given <code>\( \mathbf{O} \)</code> with the time <code>\( t \)</code> from <code>\( 1 \)</code> to <code>\( n \)</code>.</p>

<p>So let's start from scratch.
Initially, when <code>\( t = 0 \)</code>, we have no observations. Therefore, the only thing we could do is guess the next hidden state based on the prior knowledge
<code>\[
\mathbf{P}(X_1) = \pi
\]</code>
Then, <code>\( t \)</code> move on to <code>\( 1 \)</code>, we observe our first observation <code>\( O_1 \)</code>. The Bayesian posterior probability by adding this new information is,
<code>\[
\mathbf{P}(X_1 | O_1) = \frac{\mathbf{P}(O_1 | X_1) \mathbf{P}(X_1)}{ \sum_{x} \mathbf{P}(O_1|x)\mathbf{P} (x)}
\]</code>
following that, we are also curious a little bit more about the next state based on the newly updated observations.
Because of Markov property, the probability of next state <code>\( X_2 \)</code> only depends on <code>\( X_1 \)</code>.
<code>\[
\mathbf{P}(X_2 | O_1) = \sum_{x_1} \mathbf{P}(X_2 | x_1) \mathbf{P}(x_1 | O_1)
\]</code></p>

<p>We could follow above computation iteratively through time.
Essentially, for each time step, there are two steps involve in the iteration,</p>

<ul>
<li>Prior Probability
<code>\[ \mathbf{P} (X_{k+1} | O_{1:k}) = \sum_{x_k} \mathbf{P}(X_{k+1} | x_k) \mathbf{P}(x_k | O_{1:k}) \]</code></li>
<li>Posterior Probability
<code>\[ \mathbf{P} (X_{k+1} | O_{1:k+1}) = \frac{\mathbf{P}(O_{k+1} | X_{k+1}) \mathbf{P}(X_{k+1} | O_{1:k})}{ \sum_{x} \mathbf{P}(O_{k+1}|x)\mathbf{P} (x | O_{1:k})}\]</code></li>
</ul>


<p>Under different restrictions, the above estimation commonly have two filtering method, Kalman filter and Particle filter.</p>

<ul>
<li>Kalman filter.
  When the process <code>\( \mathbf{X} \)</code> is Gaussian linear process, by taking advantage of Gaussian property, both prior and posterior calculations have Gaussian close form.</li>
<li>Particle filter
  When the process <code>\( \mathbf{X} \)</code> is not Gaussian linear process,
  the posterior probability no longer have close form to calculate.
  Monte Carlo simulation (Sequential importance re-sampling) will be used to estimate the distribution.</li>
</ul>


</div>

    			</div> 
    		</div>
    	</div>
      <div class="footer">
        <div class="link">
          <a class="icon" href="http://github.com/xingzhong" style="color:#3b8bba">
             <i class="fa fa-github fa-2x"></i> 
          </a>
          <a class="icon" href="http://www.linkedin.com/in/xingzhongxu" style="color:#0077b5;">
             <i class="fa fa-linkedin fa-2x"></i> 
          </a>
          <a class="icon" href="http://www.twitter.com/xxingzhong" style="color:#0084B4;">
             <i class="fa fa-twitter fa-2x"></i> 
          </a>
          <a class="icon" href="http://weibo.com/xxingzhong" style="color:#e6162d;">
             <i class="fa fa-weibo fa-2x"></i> 
          </a>
          <a class="icon" href="http://www.facebook.com/xingzhong" style="color:#3b5998;">
             <i class="fa fa-facebook fa-2x"></i> 
          </a>
          <a class="icon" href="http://google.com/+XingzhongXu" style="color:#e46f61;">
             <i class="fa fa-google-plus fa-2x"></i> 
          </a>
          <a class="icon" href="http://xueqiu.com/sirxu">
            <img style="height:30px; position:relative; top:6px;"src="/assets/xueqiu.png">
          </a>
        </div>
        <p>Xingzhong 2014</p>
      </div>
    </body>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-16398465-1', 'sirxu.com');
      ga('send', 'pageview');

    </script>
    <script type="text/javascript">
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.0/jquery.min.js"></script>
    
    <script src="/js/main.js"></script>

</html>
